{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "THIS IS A MINI PROJECT DONE BY \n",
        "\n",
        "\n",
        "\n",
        "1.   BHANU GUPTA\n",
        "2.   PRANAV NARAYAN\n",
        "3.   PURVAV PUNYANI\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gMmGpdF3RU46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import all the required models "
      ],
      "metadata": {
        "id": "VOJAWkXbROsO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYXsoW4pMfLw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: This code creates a building block for Resnet that will help the network to learn more efficiently. It uses two layers of image filters to process the input image and apply some mathematical operations to it. The output of the second layer is added to the input image, which helps the neural network to better understand the data. \n",
        "\n",
        "The basic building block is common in most resnet models "
      ],
      "metadata": {
        "id": "WiYiYUrKSHMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class block(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(block, self).__init__()\n",
        "      \n",
        "        self.conv1 = nn.Conv2d(in_planes\n",
        "                               , planes, \n",
        "                               kernel_size=3, \n",
        "                               stride=stride, \n",
        "                               padding=1, \n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        \n",
        "        \n",
        "        self.conv2 = nn.Conv2d(planes, \n",
        "                               planes, \n",
        "                               kernel_size=3, \n",
        "                               stride=1, \n",
        "                               padding=1, \n",
        "                               bias=False)\n",
        "        \n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(residual)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "zx0fxaaBMhY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: In this step we modify the ResNet model to keep the number of parameters within 5 million. The modified ResNet architecture has four layers, each made up of a variable number of residual blocks (defined by the num_blocks parameter). Each residual block is made up of two or three convolutional layers, followed by batch normalization and a ReLU activation function. The final layer is a fully connected layer that outputs the predicted class probabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "g3DS3zHPSqMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MOD_ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(MOD_ResNet, self).__init__()\n",
        "\n",
        "\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        layers = []\n",
        "        for i in range(num_blocks):\n",
        "            if i == 0:\n",
        "                layers.append(block(self.in_planes, planes, stride))\n",
        "            else:\n",
        "                layers.append(block(self.in_planes, planes, 1))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        \n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        \n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "def ResNet_Cifar():\n",
        "    return MOD_ResNet(block, [3, 4, 6, 2])"
      ],
      "metadata": {
        "id": "KkYC8d0iSDTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4: WE NOW USE torch.summary to find out the total number of parameters and make sure the total is less than 5 million. In our case the total parameter count is 4,158,090. "
      ],
      "metadata": {
        "id": "0LkruBoITU-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Set the device to use for training (CUDA if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet_Cifar().cuda()\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev5TBmtBOa1Z",
        "outputId": "91b0a238-cc38-4258-9ba0-ea8f53583f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 32, 32, 32]          18,432\n",
            "       BatchNorm2d-4           [-1, 32, 32, 32]              64\n",
            "            Conv2d-5           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-6           [-1, 32, 32, 32]              64\n",
            "            Conv2d-7           [-1, 32, 32, 32]           2,048\n",
            "       BatchNorm2d-8           [-1, 32, 32, 32]              64\n",
            "             block-9           [-1, 32, 32, 32]               0\n",
            "           Conv2d-10           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-11           [-1, 32, 32, 32]              64\n",
            "           Conv2d-12           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-13           [-1, 32, 32, 32]              64\n",
            "            block-14           [-1, 32, 32, 32]               0\n",
            "           Conv2d-15           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-16           [-1, 32, 32, 32]              64\n",
            "           Conv2d-17           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-18           [-1, 32, 32, 32]              64\n",
            "            block-19           [-1, 32, 32, 32]               0\n",
            "           Conv2d-20           [-1, 64, 16, 16]          18,432\n",
            "      BatchNorm2d-21           [-1, 64, 16, 16]             128\n",
            "           Conv2d-22           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 16, 16]             128\n",
            "           Conv2d-24           [-1, 64, 16, 16]           2,048\n",
            "      BatchNorm2d-25           [-1, 64, 16, 16]             128\n",
            "            block-26           [-1, 64, 16, 16]               0\n",
            "           Conv2d-27           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-28           [-1, 64, 16, 16]             128\n",
            "           Conv2d-29           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-30           [-1, 64, 16, 16]             128\n",
            "            block-31           [-1, 64, 16, 16]               0\n",
            "           Conv2d-32           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 16, 16]             128\n",
            "           Conv2d-34           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 16, 16]             128\n",
            "            block-36           [-1, 64, 16, 16]               0\n",
            "           Conv2d-37           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 16, 16]             128\n",
            "           Conv2d-39           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-40           [-1, 64, 16, 16]             128\n",
            "            block-41           [-1, 64, 16, 16]               0\n",
            "           Conv2d-42            [-1, 128, 8, 8]          73,728\n",
            "      BatchNorm2d-43            [-1, 128, 8, 8]             256\n",
            "           Conv2d-44            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-45            [-1, 128, 8, 8]             256\n",
            "           Conv2d-46            [-1, 128, 8, 8]           8,192\n",
            "      BatchNorm2d-47            [-1, 128, 8, 8]             256\n",
            "            block-48            [-1, 128, 8, 8]               0\n",
            "           Conv2d-49            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 8, 8]             256\n",
            "           Conv2d-51            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-52            [-1, 128, 8, 8]             256\n",
            "            block-53            [-1, 128, 8, 8]               0\n",
            "           Conv2d-54            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-55            [-1, 128, 8, 8]             256\n",
            "           Conv2d-56            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-57            [-1, 128, 8, 8]             256\n",
            "            block-58            [-1, 128, 8, 8]               0\n",
            "           Conv2d-59            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-60            [-1, 128, 8, 8]             256\n",
            "           Conv2d-61            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-62            [-1, 128, 8, 8]             256\n",
            "            block-63            [-1, 128, 8, 8]               0\n",
            "           Conv2d-64            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-65            [-1, 128, 8, 8]             256\n",
            "           Conv2d-66            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-67            [-1, 128, 8, 8]             256\n",
            "            block-68            [-1, 128, 8, 8]               0\n",
            "           Conv2d-69            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-70            [-1, 128, 8, 8]             256\n",
            "           Conv2d-71            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-72            [-1, 128, 8, 8]             256\n",
            "            block-73            [-1, 128, 8, 8]               0\n",
            "           Conv2d-74            [-1, 256, 4, 4]         294,912\n",
            "      BatchNorm2d-75            [-1, 256, 4, 4]             512\n",
            "           Conv2d-76            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-77            [-1, 256, 4, 4]             512\n",
            "           Conv2d-78            [-1, 256, 4, 4]          32,768\n",
            "      BatchNorm2d-79            [-1, 256, 4, 4]             512\n",
            "            block-80            [-1, 256, 4, 4]               0\n",
            "           Conv2d-81            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-82            [-1, 256, 4, 4]             512\n",
            "           Conv2d-83            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-84            [-1, 256, 4, 4]             512\n",
            "            block-85            [-1, 256, 4, 4]               0\n",
            "AdaptiveAvgPool2d-86            [-1, 256, 1, 1]               0\n",
            "           Linear-87                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,158,090\n",
            "Trainable params: 4,158,090\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 10.38\n",
            "Params size (MB): 15.86\n",
            "Estimated Total Size (MB): 26.25\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 6: We define and apply data augmentation and normalization steps to the CIFAR-10 dataset for training and testing. The datasets are loaded using DataLoader with batch size, shuffle, and number of workers specified."
      ],
      "metadata": {
        "id": "XrGKpVmjTmiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data augmentation and normalization steps\n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop(32, padding=4),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load the CIFAR-10 dataset for training\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load the CIFAR-10 dataset for testing\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO3KT1uER16r",
        "outputId": "139df1c6-d130-4bf6-9458-7c157e2e4d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 13099647.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "model = ResNet_Cifar().to(device)\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Iterate over the training data batches\n",
        "    for i, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {running_loss/(i+1)}, Train accuracy: {100*correct/total}\")\n",
        "    return running_loss / (i + 1), 100 * correct / total\n",
        "\n",
        "# Define the testing function for one epoch\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Iterate over the test data batches\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    # Print the test accuracy for this epoch\n",
        "    print(f\"        Test accuracy: {100*correct/total}\")\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "    return test_loss / (batch_idx + 1), 100 * correct / total\n",
        "\n",
        "# Set the number of epochs for training\n",
        "num_epochs = 100\n",
        "\n",
        "# Train and test the model for the specified number of epochs\n",
        "for epoch in range(num_epochs): \n",
        "    train_loss, train_acc = train(epoch)  \n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    test_loss, test_acc = test(epoch)  \n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "    scheduler.step()  "
      ],
      "metadata": {
        "id": "sN8XRIXbSPAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965c1776-fa12-47ba-a2f2-4a05fb6b4b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.4777697069644928, Train accuracy: 45.608\n",
            "        Test accuracy: 54.41\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 2, Loss: 1.0413982495069505, Train accuracy: 62.834\n",
            "        Test accuracy: 58.54\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 3, Loss: 0.852825511097908, Train accuracy: 70.114\n",
            "        Test accuracy: 72.45\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 4, Loss: 0.7261235148310662, Train accuracy: 74.976\n",
            "        Test accuracy: 70.69\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 5, Loss: 0.6487267102599144, Train accuracy: 77.524\n",
            "        Test accuracy: 77.41\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 6, Loss: 0.5899598770737648, Train accuracy: 79.876\n",
            "        Test accuracy: 74.3\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 7, Loss: 0.5408741812109947, Train accuracy: 81.504\n",
            "        Test accuracy: 79.49\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 8, Loss: 0.5070032513737679, Train accuracy: 82.72\n",
            "        Test accuracy: 81.72\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 9, Loss: 0.4763363512158394, Train accuracy: 83.586\n",
            "        Test accuracy: 79.88\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 10, Loss: 0.45947650280594826, Train accuracy: 84.41\n",
            "        Test accuracy: 81.5\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 11, Loss: 0.43547512608766553, Train accuracy: 85.19\n",
            "        Test accuracy: 83.77\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 12, Loss: 0.41541946136951446, Train accuracy: 85.816\n",
            "        Test accuracy: 78.17\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 13, Loss: 0.4034721011221409, Train accuracy: 86.212\n",
            "        Test accuracy: 82.91\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 14, Loss: 0.38325390142202376, Train accuracy: 86.942\n",
            "        Test accuracy: 85.94\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 15, Loss: 0.3739481965303421, Train accuracy: 87.192\n",
            "        Test accuracy: 84.73\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 16, Loss: 0.36258634087443353, Train accuracy: 87.514\n",
            "        Test accuracy: 85.52\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 17, Loss: 0.35956248360872267, Train accuracy: 87.676\n",
            "        Test accuracy: 82.26\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 18, Loss: 0.3495093549787998, Train accuracy: 88.05\n",
            "        Test accuracy: 85.96\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 19, Loss: 0.3380567454099655, Train accuracy: 88.506\n",
            "        Test accuracy: 86.32\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 20, Loss: 0.330295006275177, Train accuracy: 88.782\n",
            "        Test accuracy: 84.95\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 21, Loss: 0.3272554123401642, Train accuracy: 88.824\n",
            "        Test accuracy: 86.41\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 22, Loss: 0.32390864557027815, Train accuracy: 88.91\n",
            "        Test accuracy: 85.16\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 23, Loss: 0.3136046492010355, Train accuracy: 89.376\n",
            "        Test accuracy: 83.04\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 24, Loss: 0.3070688585937023, Train accuracy: 89.382\n",
            "        Test accuracy: 86.65\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 25, Loss: 0.30084656894207, Train accuracy: 89.67\n",
            "        Test accuracy: 85.96\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 26, Loss: 0.29902017560601235, Train accuracy: 89.682\n",
            "        Test accuracy: 84.57\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 27, Loss: 0.2971149652153254, Train accuracy: 89.772\n",
            "        Test accuracy: 87.32\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 28, Loss: 0.29318408294022086, Train accuracy: 89.932\n",
            "        Test accuracy: 87.22\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 29, Loss: 0.28854279275238515, Train accuracy: 90.106\n",
            "        Test accuracy: 86.55\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 30, Loss: 0.2786149129867554, Train accuracy: 90.266\n",
            "        Test accuracy: 87.42\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 31, Loss: 0.2794795495867729, Train accuracy: 90.37\n",
            "        Test accuracy: 87.48\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 32, Loss: 0.2764653975367546, Train accuracy: 90.384\n",
            "        Test accuracy: 86.36\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 33, Loss: 0.2734078210443258, Train accuracy: 90.598\n",
            "        Test accuracy: 87.04\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 34, Loss: 0.2698560347110033, Train accuracy: 90.712\n",
            "        Test accuracy: 88.62\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 35, Loss: 0.26629612462222574, Train accuracy: 90.852\n",
            "        Test accuracy: 85.15\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 36, Loss: 0.2700324002802372, Train accuracy: 90.71\n",
            "        Test accuracy: 87.33\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 37, Loss: 0.2590279398113489, Train accuracy: 91.068\n",
            "        Test accuracy: 87.9\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 38, Loss: 0.26171965204179287, Train accuracy: 90.976\n",
            "        Test accuracy: 86.77\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 39, Loss: 0.2554464366734028, Train accuracy: 91.306\n",
            "        Test accuracy: 88.38\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 40, Loss: 0.2560872753560543, Train accuracy: 91.202\n",
            "        Test accuracy: 87.36\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 41, Loss: 0.24792834627628327, Train accuracy: 91.466\n",
            "        Test accuracy: 87.02\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 42, Loss: 0.2478495252430439, Train accuracy: 91.52\n",
            "        Test accuracy: 87.02\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 43, Loss: 0.24411778417229651, Train accuracy: 91.606\n",
            "        Test accuracy: 87.55\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 44, Loss: 0.24305532115697862, Train accuracy: 91.634\n",
            "        Test accuracy: 88.17\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 45, Loss: 0.2442343515008688, Train accuracy: 91.556\n",
            "        Test accuracy: 86.47\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 46, Loss: 0.2413706226348877, Train accuracy: 91.77\n",
            "        Test accuracy: 87.12\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 47, Loss: 0.232932405680418, Train accuracy: 91.998\n",
            "        Test accuracy: 86.46\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 48, Loss: 0.23520581382513045, Train accuracy: 91.972\n",
            "        Test accuracy: 88.36\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 49, Loss: 0.23455294492840767, Train accuracy: 91.928\n",
            "        Test accuracy: 87.42\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 50, Loss: 0.23097033251821994, Train accuracy: 92.108\n",
            "        Test accuracy: 88.25\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 51, Loss: 0.2279811798930168, Train accuracy: 92.134\n",
            "        Test accuracy: 87.37\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 52, Loss: 0.22582392093539239, Train accuracy: 92.168\n",
            "        Test accuracy: 86.58\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 53, Loss: 0.22550444088876248, Train accuracy: 92.348\n",
            "        Test accuracy: 88.68\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 54, Loss: 0.22391391043365003, Train accuracy: 92.302\n",
            "        Test accuracy: 87.71\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 55, Loss: 0.22024411492049695, Train accuracy: 92.48\n",
            "        Test accuracy: 87.95\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 56, Loss: 0.2188111022412777, Train accuracy: 92.458\n",
            "        Test accuracy: 88.9\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 57, Loss: 0.21471934378147126, Train accuracy: 92.63\n",
            "        Test accuracy: 87.98\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 58, Loss: 0.21185599486529827, Train accuracy: 92.816\n",
            "        Test accuracy: 87.78\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 59, Loss: 0.21059847521781921, Train accuracy: 92.818\n",
            "        Test accuracy: 87.89\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 60, Loss: 0.211557602211833, Train accuracy: 92.702\n",
            "        Test accuracy: 88.46\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 61, Loss: 0.2090733683258295, Train accuracy: 92.908\n",
            "        Test accuracy: 88.01\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 62, Loss: 0.20674643759429454, Train accuracy: 92.93\n",
            "        Test accuracy: 88.96\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 63, Loss: 0.20403030906617642, Train accuracy: 93.038\n",
            "        Test accuracy: 88.89\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 64, Loss: 0.19769965778291226, Train accuracy: 93.33\n",
            "        Test accuracy: 89.45\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 65, Loss: 0.2003019322901964, Train accuracy: 92.966\n",
            "        Test accuracy: 88.48\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 66, Loss: 0.1952668353766203, Train accuracy: 93.318\n",
            "        Test accuracy: 90.13\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 67, Loss: 0.19682747136056422, Train accuracy: 93.19\n",
            "        Test accuracy: 89.04\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 68, Loss: 0.19130903244763614, Train accuracy: 93.368\n",
            "        Test accuracy: 89.62\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 69, Loss: 0.18550191755592824, Train accuracy: 93.64\n",
            "        Test accuracy: 88.98\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 70, Loss: 0.19097017734497784, Train accuracy: 93.48\n",
            "        Test accuracy: 88.41\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 71, Loss: 0.18797284009307622, Train accuracy: 93.574\n",
            "        Test accuracy: 89.58\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 72, Loss: 0.18108926396816968, Train accuracy: 93.838\n",
            "        Test accuracy: 90.1\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 73, Loss: 0.18292733269929887, Train accuracy: 93.638\n",
            "        Test accuracy: 88.99\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 74, Loss: 0.18214851393550635, Train accuracy: 93.746\n",
            "        Test accuracy: 89.27\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 75, Loss: 0.17822051309794187, Train accuracy: 93.8\n",
            "        Test accuracy: 88.74\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 76, Loss: 0.17863747394829987, Train accuracy: 93.9\n",
            "        Test accuracy: 88.7\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 77, Loss: 0.17497195506095886, Train accuracy: 94.014\n",
            "        Test accuracy: 89.31\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 78, Loss: 0.1763389874920249, Train accuracy: 93.916\n",
            "        Test accuracy: 89.14\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 79, Loss: 0.17219570878893137, Train accuracy: 94.066\n",
            "        Test accuracy: 89.05\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 80, Loss: 0.16773486460745335, Train accuracy: 94.19\n",
            "        Test accuracy: 89.23\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 81, Loss: 0.16884038123488426, Train accuracy: 94.156\n",
            "        Test accuracy: 89.94\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 82, Loss: 0.16469881879538298, Train accuracy: 94.342\n",
            "        Test accuracy: 89.28\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 83, Loss: 0.16352610593289138, Train accuracy: 94.324\n",
            "        Test accuracy: 89.79\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 84, Loss: 0.16302299542725085, Train accuracy: 94.396\n",
            "        Test accuracy: 88.62\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 85, Loss: 0.15942758885771036, Train accuracy: 94.61\n",
            "        Test accuracy: 88.77\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 86, Loss: 0.15963739501684904, Train accuracy: 94.46\n",
            "        Test accuracy: 89.25\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 87, Loss: 0.15760635459423064, Train accuracy: 94.488\n",
            "        Test accuracy: 90.01\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 88, Loss: 0.15342625059187412, Train accuracy: 94.762\n",
            "        Test accuracy: 89.41\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 89, Loss: 0.15470960568636655, Train accuracy: 94.764\n",
            "        Test accuracy: 90.03\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 90, Loss: 0.1501066023670137, Train accuracy: 94.774\n",
            "        Test accuracy: 89.98\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 91, Loss: 0.14816482053697108, Train accuracy: 94.862\n",
            "        Test accuracy: 89.74\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 92, Loss: 0.14698671206086875, Train accuracy: 94.97\n",
            "        Test accuracy: 89.61\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 93, Loss: 0.14285461530089377, Train accuracy: 95.11\n",
            "        Test accuracy: 90.19\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 94, Loss: 0.14016777831315994, Train accuracy: 95.094\n",
            "        Test accuracy: 89.07\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 95, Loss: 0.1406984887868166, Train accuracy: 95.118\n",
            "        Test accuracy: 89.87\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 96, Loss: 0.13590534491091966, Train accuracy: 95.244\n",
            "        Test accuracy: 90.52\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 97, Loss: 0.13308742076531052, Train accuracy: 95.404\n",
            "        Test accuracy: 89.69\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 98, Loss: 0.12948023956641555, Train accuracy: 95.504\n",
            "        Test accuracy: 90.64\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 99, Loss: 0.13201922711730002, Train accuracy: 95.458\n",
            "        Test accuracy: 89.36\n",
            "-------------------------------------------------------------------------\n",
            "Epoch: 100, Loss: 0.128330313436687, Train accuracy: 95.51\n",
            "        Test accuracy: 90.72\n",
            "-------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}